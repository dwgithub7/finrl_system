import numpy as np
from finrl.training.trading_env import TradingEnv
from finrl.ensemble.dqn_agent import DQNAgent

import torch
import os
from torch.utils.data import DataLoader, TensorDataset

# ì‹¤ì „ ë°ì´í„° ê²½ë¡œ
DATA_DIR = "data/dualbranch"
X_minute = np.load(os.path.join(DATA_DIR, "SOLUSDT_1m_finrl_20250101~20250426_X_minute_full.npy"))
X_daily = np.load(os.path.join(DATA_DIR, "SOLUSDT_1m_finrl_20250101~20250426_X_daily_full.npy"))
y_entry = np.load(os.path.join(DATA_DIR, "SOLUSDT_1m_finrl_20250101~20250426_y_entry_full.npy"))
y_direction = np.load(os.path.join(DATA_DIR, "SOLUSDT_1m_finrl_20250101~20250426_y_direction_full.npy"))

# í™˜ê²½ ì´ˆê¸°í™”
env = TradingEnv(X_minute, X_daily, y_entry, y_direction)
# state_size = env.observation_space.shape[0]
state_size = X_minute.shape[1] * X_minute.shape[2] + X_daily.shape[1] * X_daily.shape[2]
# ì¦‰: 30*20 + 10*20 = 800action_size = env.action_space.n
action_size = 4

# ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
agent = DQNAgent(state_size=state_size, action_size=action_size)

print("âœ… X_minute shape:", X_minute.shape)
print("âœ… X_daily shape:", X_daily.shape)
print("âœ… state_size:", state_size)
print("âœ… model input dim:", agent.model.model[0].in_features)

# í•™ìŠµ íŒŒë¼ë¯¸í„°
episodes = 300  # ì¡°ì ˆí•„ìš”

agent.memory.clear()  # ğŸ§¹ ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì¶”ê°€ (ê¸°ì¡´ì— ì˜ëª» ì €ì¥ëœ ë°ì´í„° ì œê±°)

for ep in range(episodes):
    state = env.reset()
    total_reward = 0
    done = False
    while not done:
        action = agent.act(state)
        next_state, reward, done, _ = env.step(action)
        agent.remember(state, action, reward, next_state, done)
        state = next_state
        total_reward += reward
    agent.replay()
    print(f"EP {ep + 1}/{episodes}, Total Reward: {total_reward:.2f}")

# ëª¨ë¸ ì €ì¥
agent.save("models/dqn_model.pt")